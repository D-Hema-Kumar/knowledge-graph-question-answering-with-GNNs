{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\I583773\\.virtualenvs\\psmg-imkg-gnn-qa-b7xnrL-L\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from loguru import logger\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from core.LLM.LLMEncoder import LLMEncoder\n",
    "from core.ToTorch.DataBuilder import QADataBuilder, QAMaskBuilder\n",
    "from config.config import (\n",
    "    TRIPLES_PATH,\n",
    "    ENTITIES_LABELS_PATH,\n",
    "    PROPERTIES_LABELS_PATH,\n",
    "    GRAPH_EMBEDDINGS_PATH,\n",
    "    QUESTIONS_ANSWERS_PATH,\n",
    "    QUESTIONS_EMBEDDINGS_PATH,\n",
    "    QUESTIONS_CONCEPTS_ANSWERS_PATH,\n",
    "    GRAPH_EMBEDDINGS_WITH_COMMENT_PATH,\n",
    "    EXPERIMENT_RESULTS_PATH,\n",
    "\n",
    "    TRIPLES_PATH_OLD,\n",
    "    ENTITIES_LABELS_PATH_OLD,\n",
    "    PROPERTIES_LABELS_PATH_OLD,\n",
    "    GRAPH_EMBEDDINGS_PATH_OLD,\n",
    "    QUESTIONS_CONCEPTS_ANSWERS_PATH,\n",
    "    GRAPH_EMBEDDINGS_PATH_OLD, QA_TRAINING_FILE_PATH,\n",
    "    QA_TESTING_FILE_PATH\n",
    ")\n",
    "\n",
    "from core.NeuralNet.GNN import GCN,RGCN\n",
    "\n",
    "from core.experiments.utils import QAEvaluationMetrcis, load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "NUM_EPOCHS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example graph with 15 nodes but just 5 edges where many nodes from node 4 to node 14 are not connected with anything.\n",
    "x = torch.rand(15,5)\n",
    "edge_index = torch.tensor([[0, 1, 12, 12, 13], [1, 2, 13, 10, 11]], dtype=torch.long)\n",
    "edge_type = torch.tensor([0, 1, 2, 0, 3], dtype=torch.long)\n",
    "\n",
    "# Assuming node_type_ids is [0, 1, 2, 3]\n",
    "node_type_ids = torch.tensor([0, 1, 2, 3], dtype=torch.long)\n",
    "\n",
    "data = Data(x=x,edge_index=edge_index, edge_type=edge_type)\n",
    "print('edge_index:',edge_index)\n",
    "#x\n",
    "#r_model = RGCN(num_node_features=5,dim_hidden_layer=2,num_relations=4,num_bases=None,num_layers=2,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_mask = [False,False,True,True,True]\n",
    "\n",
    "masked_edge_index = edge_index[:,q_mask]\n",
    "print('masked_edge_index:', masked_edge_index)\n",
    "masked_edge_type = edge_type[q_mask]\n",
    "print('masked_edge_type:', masked_edge_type)\n",
    "q_nodes = torch.tensor([10,13,11,12])\n",
    "x[q_nodes]\n",
    "q_nodes_to_index = {sub_node : index for index, sub_node in enumerate (q_nodes.tolist())}\n",
    "print('mapping dict:',q_nodes_to_index)\n",
    "\n",
    "mapped_masked_edge_index = torch.tensor(\n",
    "    [\n",
    "       [q_nodes_to_index[head.item()] for head in masked_edge_index[0]],\n",
    "       [q_nodes_to_index[tail.item()] for tail in masked_edge_index[1]]\n",
    "    ],dtype=torch.long\n",
    ")\n",
    "mapped_masked_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_data_builder = QAMaskBuilder(\n",
    "    triples_path=TRIPLES_PATH_OLD,\n",
    "    entities_labels_path=ENTITIES_LABELS_PATH_OLD,\n",
    "    properties_labels_path=PROPERTIES_LABELS_PATH_OLD,\n",
    "    embeddings_path=GRAPH_EMBEDDINGS_PATH_OLD,\n",
    "    training_questions_concepts_answers_file_path = QA_TRAINING_FILE_PATH,\n",
    "    testing_questions_concepts_answers_file_path = QA_TESTING_FILE_PATH,\n",
    "    questions_embeddings_path = QUESTIONS_EMBEDDINGS_PATH\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = qa_data_builder.build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN GNN\n",
    "logger.info(\"Training GNN\")\n",
    "model = GCN(\n",
    "    num_node_features=data.num_node_features*2, dim_hidden_layer=16,num_layers=1, num_classes=2\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in qa_data_builder.training_questions_concepts_answers.iterrows():\n",
    "    #print(row[\"question\"])\n",
    "    q_data = qa_data_builder.get_concepts_and_masks_for_question(\n",
    "                                    question =row[\"question\"], \n",
    "                                    concept_uri= row[\"concepts\"], \n",
    "                                    answer_uri= row[\"answers\"], \n",
    "                                    training=True)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    out,embedding = model(q_data)\n",
    "    loss = F.nll_loss(out[q_data.train_mask], q_data.y[q_data.train_mask],weight = torch.tensor([1.0,15.0]))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    logger.debug(f\"Epoch: {idx:03d}, Loss: {loss:.4f}\")\n",
    "   \n",
    "    break;\n",
    "q_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_data.y[q_data.train_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-04 17:15:51.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mCreating Data object\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "QAMaskBuilder.__init__() got an unexpected keyword argument 'questions_concepts_answers_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m## CREATE DATA\u001b[39;00m\n\u001b[0;32m      6\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCreating Data object\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m qa_data_builder \u001b[39m=\u001b[39m QAMaskBuilder(\n\u001b[0;32m      9\u001b[0m     triples_path\u001b[39m=\u001b[39;49mTRIPLES_PATH_OLD,\n\u001b[0;32m     10\u001b[0m     entities_labels_path\u001b[39m=\u001b[39;49mENTITIES_LABELS_PATH_OLD,\n\u001b[0;32m     11\u001b[0m     properties_labels_path\u001b[39m=\u001b[39;49mPROPERTIES_LABELS_PATH_OLD,\n\u001b[0;32m     12\u001b[0m     embeddings_path\u001b[39m=\u001b[39;49mGRAPH_EMBEDDINGS_PATH_OLD,\n\u001b[0;32m     13\u001b[0m     questions_concepts_answers_path\u001b[39m=\u001b[39;49mQUESTIONS_CONCEPTS_ANSWERS_PATH,\n\u001b[0;32m     14\u001b[0m     questions_embeddings_path \u001b[39m=\u001b[39;49m QUESTIONS_EMBEDDINGS_PATH\n\u001b[0;32m     15\u001b[0m     \n\u001b[0;32m     16\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: QAMaskBuilder.__init__() got an unexpected keyword argument 'questions_concepts_answers_path'"
     ]
    }
   ],
   "source": [
    "# Build data\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"DEBUG\")\n",
    "\n",
    "## CREATE DATA\n",
    "logger.info(\"Creating Data object\")\n",
    "\n",
    "qa_data_builder = QAMaskBuilder(\n",
    "    triples_path=TRIPLES_PATH_OLD,\n",
    "    entities_labels_path=ENTITIES_LABELS_PATH_OLD,\n",
    "    properties_labels_path=PROPERTIES_LABELS_PATH_OLD,\n",
    "    embeddings_path=GRAPH_EMBEDDINGS_PATH_OLD,\n",
    "    questions_concepts_answers_path=QUESTIONS_CONCEPTS_ANSWERS_PATH,\n",
    "    questions_embeddings_path = QUESTIONS_EMBEDDINGS_PATH\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = qa_data_builder.build_data()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN GNN\n",
    "logger.info(\"Training GNN\")\n",
    "model = GCN(\n",
    "    num_node_features=data.num_node_features*2, dim_hidden_layer=16,num_layers=1, num_classes=2\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_subgraph_info = []\n",
    "for idx, row in tqdm(qa_data_builder.question_concepts_answers.iterrows()):\n",
    "\n",
    "    q_edge_mask, q_nodes, q_concept_mask, q_answer_mask, q_answer_and_random_nodes_mask =qa_data_builder.get_concepts_and_masks_for_question(question =row[\"question\"], concept_uri= row[\"concepts\"], answer_uri= row[\"answers\"])\n",
    "    q_edge_index = data.edge_index[:,q_edge_mask]\n",
    "    q_edge_type = data.edge_type[q_edge_mask]\n",
    "    q_training_x_mask = qa_data_builder.get_question_training_mask_for_x()\n",
    "    q_y_labels = qa_data_builder.get_question_y_labels()\n",
    "    qa_subgraph_info.append({\"q_idx\":idx,\"q\":row[\"question\"],\"q_pattern_id\":row[\"pattern_id\"],\"q_edge_mask\":q_edge_mask, \"q_nodes\":q_nodes, \"q_concept_mask\":q_concept_mask, \"q_answer_mask\":q_answer_mask, \"q_answer_and_random_nodes_mask\":q_answer_and_random_nodes_mask,\"q_edge_index\":q_edge_index,\"q_edge_type\":q_edge_type,\"q_training_x_mask\":q_training_x_mask,\"q_y_labels\":q_y_labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_subgraph_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No of questions that have answer node within it\n",
    "count_answers_in_subgraph = 0\n",
    "for item in tqdm(qa_subgraph_info):\n",
    "    if sum(item[\"q_answer_mask\"])>0:\n",
    "        count_answers_in_subgraph += 1\n",
    "count_answers_in_subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_id_q_idx_node_count  = []\n",
    "for idx, item in tqdm(enumerate(qa_subgraph_info)):\n",
    "    node_count = len(item[\"q_nodes\"])\n",
    "    pattern_id = item['q_pattern_id']\n",
    "    q_idx = item['q_idx']\n",
    "    pattern_id_q_idx_node_count.append((pattern_id,q_idx,node_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "counts = defaultdict(dict)\n",
    "\n",
    "for pattern_id, question_index, nodes_in_question in pattern_id_q_idx_node_count:\n",
    "    counts[pattern_id][question_index]= nodes_in_question\n",
    "\n",
    "# Create subplots with 5 plots per row and 3 rows\n",
    "num_patterns = len(counts)\n",
    "rows = 5\n",
    "cols = num_patterns // rows + (1 if num_patterns % rows > 0 else 0)\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "# Create plots for each pattern_id\n",
    "for i, (pattern_id, question_counts) in enumerate(counts.items()):\n",
    "\n",
    "    row = i // cols\n",
    "    col = i % cols\n",
    "    x = list(question_counts.keys())\n",
    "    y = list(question_counts.values())\n",
    "\n",
    "    ax = axes[row, col]\n",
    "    ax.bar(x, y)\n",
    "    ax.set_xlabel('Question Index')\n",
    "    ax.set_ylabel('Subgraph Node Count')\n",
    "    ax.set_title(f'Pattern: {pattern_id} -- #Qs {len(question_counts)} -- #Nodes(Min:{min(y)} Max:{max(y)})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_node_count = [element[2] for element in pattern_id_q_idx_node_count ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots with 5 plots per row and 3 rows\n",
    "num_patterns = len(counts)\n",
    "rows = 5\n",
    "cols = num_patterns // rows + (1 if num_patterns % rows > 0 else 0)\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "# Create plots for each pattern_id\n",
    "for i, (pattern_id, question_counts) in enumerate(counts.items()):\n",
    "\n",
    "    row = i // cols\n",
    "    col = i % cols\n",
    "    subgraph_node_count = list(question_counts.values())\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    ax.hist(subgraph_node_count, bins=20, edgecolor='black')\n",
    "    ax.set_xlabel('Subgraph Node Count')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Pattern: {pattern_id} -- #Qs {len(question_counts)} -- #Nodes(Min:{min(subgraph_node_count)} Max:{max(subgraph_node_count)})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of nodes in each question\n",
    "q_idx_node_count  = []\n",
    "for idx, item in tqdm(enumerate(qa_subgraph_info)):\n",
    "    node_count = len(item[\"q_nodes\"])\n",
    "    q_idx_node_count.append(node_count)\n",
    "print(\"Average nodes in the subgraphs :\",round(sum(q_idx_node_count)/idx,2))\n",
    "print(\"Maximum nodes in the subgraph :\",max(q_idx_node_count))\n",
    "print(\"Minimum nodes in the subgraph :\",min(q_idx_node_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the distribution\n",
    "plt.plot(q_idx_node_count[:])\n",
    "plt.xlabel('Q_idx')\n",
    "plt.ylabel('q_idx_node_count')\n",
    "plt.title('Distribution of subgrapgh node count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.random.permutation(qa_data_builder.question_concepts_answers.index)\n",
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "limit=1\n",
    "i=0\n",
    "for idx in shuffled_indices:\n",
    "    row = qa_data_builder.question_concepts_answers.loc[idx]\n",
    "    q_embedding = qa_data_builder.questions_to_embeddings[row[\"question\"]]\n",
    "    q_x = qa_data_builder.get_x(to_concat=q_embedding)\n",
    "    q_edge_mask, q_nodes, q_concept_mask, q_answer_mask, q_answer_and_random_nodes_mask =qa_data_builder.get_concepts_and_masks_for_question(question =row[\"question\"], concept_uri= row[\"concepts\"], answer_uri= row[\"answers\"])\n",
    "    q_edge_index = data.edge_index[:,q_edge_mask]\n",
    "    q_edge_type = data.edge_type[q_edge_mask]\n",
    "    q_training_x_mask = qa_data_builder.get_question_training_mask_for_x()\n",
    "    q_y_labels = qa_data_builder.get_question_y_labels()\n",
    "    q_data = Data(x=q_x, edge_index=q_edge_index, edge_type=q_edge_type, train_mask=q_training_x_mask, y=q_y_labels)\n",
    "    print(f'Training for Q {idx} : {row[\"question\"]}')\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        out,embedding = model(q_data)\n",
    "        loss = F.nll_loss(out[q_data.train_mask], q_data.y[q_data.train_mask],weight = torch.tensor([1.0,15.0]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch%50==0:\n",
    "            logger.debug(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}\")\n",
    "    if i==limit:\n",
    "        break;\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=2\n",
    "i=0\n",
    "# Evaluation\n",
    "res = []\n",
    "for idx in shuffled_indices:\n",
    "    \n",
    "    if i==limit:\n",
    "        break;\n",
    "\n",
    "    row = qa_data_builder.question_concepts_answers.loc[idx]\n",
    "    q_embedding = qa_data_builder.questions_to_embeddings[row[\"question\"]]\n",
    "    q_x = qa_data_builder.get_x(to_concat=q_embedding)\n",
    "    q_edge_mask, q_nodes, q_concept_mask, q_answer_mask, q_answer_and_random_nodes_mask =qa_data_builder.get_concepts_and_masks_for_question(question =row[\"question\"], concept_uri= row[\"concepts\"], answer_uri= row[\"answers\"])\n",
    "    q_edge_index = data.edge_index[:,q_edge_mask]\n",
    "    q_edge_type = data.edge_type[q_edge_mask]\n",
    "    q_training_x_mask = qa_data_builder.get_question_training_mask_for_x()\n",
    "    q_y_labels = qa_data_builder.get_question_y_labels()\n",
    "    q_data = Data(x=q_x,edge_index=q_edge_index,edge_type=q_edge_type,train_mask =q_training_x_mask,y=q_y_labels)\n",
    "    model.eval()\n",
    "    print(f'Predicting for Q {idx} : {row[\"question\"]}')\n",
    "    out,_ = model(q_data)\n",
    "    predicted_answer_nodes = torch.where(out.argmax(dim=1))[0]\n",
    "    predicted_answer_node_probabilities = out.max(dim=1)[0][predicted_answer_nodes]\n",
    "    sorted_probability_indices = torch.argsort(predicted_answer_node_probabilities, descending= True)\n",
    "    count_predicted_nodes =len(predicted_answer_nodes)\n",
    "    actual_answer_nodes = q_nodes[q_answer_mask].tolist()\n",
    "    if count_predicted_nodes > 0:\n",
    "        logger.debug(f\"answers predicted\")\n",
    "        is_predicted_in_actual_answers = bool(set(actual_answer_nodes) & set(predicted_answer_nodes[sorted_probability_indices].tolist()))\n",
    "        res.append((idx, actual_answer_nodes, predicted_answer_nodes[sorted_probability_indices].tolist(),predicted_answer_node_probabilities[sorted_probability_indices].tolist(),count_predicted_nodes,is_predicted_in_actual_answers))\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        logger.debug(f\"NO answers found\")\n",
    "        res.append((idx, actual_answer_nodes, np.nan,np.nan,0,False))\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = pd.DataFrame.from_records(res,columns=[\"q_idx\",\"actual_answer_nodes\",\"predicted_answer_nodes\",\"probabilities_of_answer_nodes\",\"count_predicted_nodes\",\"is_predicted_in_actual\"])\n",
    "eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_idx = 254\n",
    "#(q_edge_mask, q_nodes, q_concept_mask, q_answer_mask, q_answer_and_random_nodes_mask,q_edge_index,q_edge_type,q_training_x_mask,q_y_labels)\n",
    "print(\"Total nodes in the subgraph\",len(set(qa_subgraph_info[q_idx]['q_nodes'].tolist())))\n",
    "\n",
    "len(set(eval_res.loc[eval_res[\"q_idx\"]==q_idx,\"predicted_answer_nodes\"].iloc[0]) & set(qa_subgraph_info[q_idx]['q_nodes'].tolist()))\n",
    "#set(eval_res.loc[eval_res[\"q_idx\"]==q_idx,\"predicted_answer_nodes\"].iloc[0]) & set(qa_subgraph_info[q_idx]['q_nodes'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=1\n",
    "i=0\n",
    "for idx, row in eval_res.iterrows():\n",
    "    if row['count_predicted_nodes'] < 30 and row['count_predicted_nodes'] >0 :\n",
    "        print(f'Question: {qa_data_builder.question_concepts_answers.loc[row[\"q_idx\"],\"question\"]}')\n",
    "        print(f'Concepts: {qa_data_builder.question_concepts_answers.loc[row[\"q_idx\"],\"concepts\"]}')\n",
    "        print(f'Actual Answer: {row[\"actual_answer_nodes\"]} -- URIs :{qa_data_builder.question_concepts_answers.loc[row[\"q_idx\"],\"answers\"]}')\n",
    "        if row[\"is_predicted_in_actual\"]:\n",
    "            for node in row[\"predicted_answer_nodes\"]:\n",
    "                \n",
    "                print(f'Preicted Node: {node} -- URI:{qa_data_builder.index_to_entity[node]}')\n",
    "        else:\n",
    "            print(\"Actua answer is not in the predicted nodes list.\")\n",
    "            for node in row[\"predicted_answer_nodes\"]:\n",
    "                print(f'Preicted Node: {node} -- URI:{qa_data_builder.index_to_entity[node]}')\n",
    "    \n",
    "    if i==limit:\n",
    "        break;\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#./core/experiments/qa/results/20230825123318/evaluation_results.csv\n",
    "\n",
    "eval_results = pd.read_csv('C:/Users/I583773/Documents/Thesis/evaluation_results.csv')\n",
    "# reads columns as lists instead of strings\n",
    "list_columns = ['predicted_answer_nodes', 'probabilities_of_answer_nodes']\n",
    "for col in list_columns:\n",
    "    eval_results[col] = eval_results[col].apply(lambda x : ast.literal_eval(x) if type(x)==str else [])\n",
    "#\\20230825123318\n",
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(eval_results.is_predicted_in_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (eval_results['count_predicted_nodes'] ==3)\n",
    "\n",
    "limit=1\n",
    "i=0\n",
    "for idx, row in eval_results[mask].iterrows():\n",
    "\n",
    "    print(f'Question: {qa_data_builder.question_concepts_answers.loc[row[\"q_idx\"],\"question\"]}')\n",
    "    print(f'Concepts: {qa_data_builder.question_concepts_answers.loc[row[\"q_idx\"],\"concepts\"]}')\n",
    "    print(f'Actual Answer: {row[\"actual_answer_nodes\"]} -- URIs :{qa_data_builder.question_concepts_answers.loc[row[\"q_idx\"],\"answers\"]}')\n",
    "    if row[\"is_predicted_in_actual\"]:\n",
    "        for node in row[\"predicted_answer_nodes\"]:\n",
    "        \n",
    "            \n",
    "            print(f'Preicted Node: {node} -- URI:{qa_data_builder.index_to_entity[node]}')\n",
    "    else:\n",
    "        print(\"Actua answer is not in the predicted nodes list.\")\n",
    "        for node in row[\"predicted_answer_nodes\"]:\n",
    "            print(f'Preicted Node: {node} -- URI:{qa_data_builder.index_to_entity[node]}')\n",
    "    \n",
    "    if i==limit:\n",
    "        break;\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAEvaluationMetrcis:\n",
    "    def __init__(self,model_prediction_path:str):\n",
    "\n",
    "        \n",
    "        self.evaluation_results = pd.read_csv(os.path.join(model_prediction_path,'evaluation_results.csv'))\n",
    "        \n",
    "        # reads columns as lists instead of strings\n",
    "        list_type_columns = ['actual_answer_nodes','predicted_answer_nodes', 'probabilities_of_answer_nodes']\n",
    "        for col in list_type_columns:\n",
    "            self.evaluation_results[col] = self.evaluation_results[col].apply(lambda x : ast.literal_eval(x) if type(x)==str else [])\n",
    "\n",
    "        #one_answer_mask = self.evaluation_results['actual_answer_nodes'].apply(lambda x : True if len(x)==1 else False )\n",
    "\n",
    "    def hits_at_k(self,predictions, actual, k):\n",
    "        hits = 0\n",
    "        for pred_nodes, actual_node in zip(predictions, actual):\n",
    "            if any(node in pred_nodes[:k] for node in actual_node):\n",
    "                hits += 1\n",
    "        return hits / len(predictions)\n",
    "\n",
    "    \n",
    "    def reciprocal_rank(self, predictions, actual):\n",
    "        ranks = []\n",
    "        for pred_nodes, actual_node in zip(predictions, actual):\n",
    "            if any(node in pred_nodes for node in actual_node):\n",
    "                rank = pred_nodes.index(actual_node[0]) + 1 if actual_node[0] in pred_nodes else 0\n",
    "                ranks.append(1 / rank if rank > 0 else 0)\n",
    "        return sum(ranks) / len(predictions)\n",
    "    \n",
    "    def precision_at_k(self,predictions, actual, k):\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for pred_nodes, actual_node in zip(predictions, actual):\n",
    "            correct_predictions += len(set(pred_nodes[:k]) & set(actual_node))\n",
    "            total_predictions += k\n",
    "        return correct_predictions / total_predictions\n",
    "    \n",
    "    def recall_at_k(self, predictions, actual, k):\n",
    "        correct_predictions = 0\n",
    "        total_actual = 0\n",
    "        for pred_nodes, actual_node in zip(predictions, actual):\n",
    "            correct_predictions += len(set(pred_nodes[:k]) & set(actual_node))\n",
    "            total_actual += len(actual_node)\n",
    "        return correct_predictions / total_actual\n",
    "\n",
    "\n",
    "\n",
    "    def run_evaluation(self):\n",
    "\n",
    "        self.hits_1 = self.hits_at_k(self.evaluation_results['predicted_answer_nodes'], self.evaluation_results['actual_answer_nodes'], k=1)\n",
    "        self.hits_3 = self.hits_at_k(self.evaluation_results['predicted_answer_nodes'], self.evaluation_results['actual_answer_nodes'], k=3)\n",
    "        self.hits_5 = self.hits_at_k(self.evaluation_results['predicted_answer_nodes'], self.evaluation_results['actual_answer_nodes'], k=5)\n",
    "        self.mrr = self.reciprocal_rank(self.evaluation_results['predicted_answer_nodes'], self.evaluation_results['actual_answer_nodes'])\n",
    "        self.recall_1 = self.recall_at_k(self.evaluation_results['predicted_answer_nodes'], self.evaluation_results['actual_answer_nodes'], k=1)\n",
    "        self.recall_3 = self.recall_at_k(self.evaluation_results['predicted_answer_nodes'], self.evaluation_results['actual_answer_nodes'], k=3)\n",
    "        self.recall_5 = self.recall_at_k(self.evaluation_results['predicted_answer_nodes'], self.evaluation_results['actual_answer_nodes'], k=5)\n",
    "        self.precision_1 = self.precision_at_k(self.evaluation_results['predicted_answer_nodes'], self.evaluation_results['actual_answer_nodes'], k=1)\n",
    "        self.precision_3 = self.precision_at_k(self.evaluation_results['predicted_answer_nodes'], self.evaluation_results['actual_answer_nodes'], k=3)\n",
    "        self.precision_5 = self.precision_at_k(self.evaluation_results['predicted_answer_nodes'], self.evaluation_results['actual_answer_nodes'], k=5)\n",
    "                \n",
    "        return self.hits_1, self.hits_3, self.hits_5, self.mrr, self.precision_1, self.precision_3 ,self.precision_5, self.recall_1, self.recall_3, self.recall_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metrics functionality\n",
    "\n",
    "#./core/experiments/qa/results/20230825123318/evaluation_results.csv\n",
    "#eval_results = pd.read_csv('C:/Users/I583773/Documents/Thesis/evaluation_results.csv')\n",
    "path = './core/experiments/qa/results/20230825123318/'\n",
    "evaluate_model = QAEvaluationMetrcis(path)\n",
    "\n",
    "hits_1, hits_3, hits_5, MRR, precision_1, precision_3, precision_5, recall_1, recall_3, recall_5 = evaluate_model.run_evaluation()\n",
    "print(f'hits@1 : {np.round(hits_1,2)} -- hits@3 : {np.round(hits_3,2)} -- hits@5 : {np.round(hits_5,2)} -- MRR : {np.round(MRR,2)}\\nprecision@1 : {np.round(precision_1,2)}, --precision@3 : {np.round(precision_3,2)} --precision@5 : {np.round(precision_5,2)}\\nrecall@1 : {np.round(recall_1,2)}, --recall@3 : {np.round(recall_3,2)} --recall@5 : {np.round(recall_5,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_answer_mask = evaluation_results['actual_answer_nodes'].apply(lambda x : True if len(x)==1 else False )\n",
    "sum(one_answer_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_at_k(evaluation_results[one_answer_mask]['predicted_answer_nodes'], evaluation_results[one_answer_mask]['actual_answer_nodes'], k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load evaluation file\n",
    "model_prediction_path = './core/experiments/qa/results/20230825123318/'\n",
    "evaluation_results = pd.read_csv(os.path.join(model_prediction_path,'evaluation_results.csv'))\n",
    "# reads columns as lists instead of strings\n",
    "list_type_columns = ['actual_answer_nodes','predicted_answer_nodes', 'probabilities_of_answer_nodes']\n",
    "for col in list_type_columns:\n",
    "    evaluation_results[col] = evaluation_results[col].apply(lambda x : ast.literal_eval(x) if type(x)==str else [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read experiment metadata\n",
    "exp_metadata = pd.read_csv('./core/experiments/qa/qa_experiments_masterdata.csv')\n",
    "exp_metadata[['time_stamp', 'Epochs', 'Learning Rate', 'hidden_dimension',\n",
    "       'num_layers', 'num_bases', 'Model', 'hits@1', 'hits@3', 'hits@5', 'mrr', 'precision@1', 'precision@3', 'precision@5', 'recall@1', 'recall@3',\n",
    "       'recall@5']].head(1)\n",
    "len(exp_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_metadata[['precision@1', 'precision@3', 'precision@5', 'recall@1', 'recall@3','recall@5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_metadata['F1@1'] = 2*(exp_metadata['precision@1']*exp_metadata['recall@1'])/(exp_metadata['precision@1']+exp_metadata['recall@1'])\n",
    "exp_metadata['F1@3'] = 2*(exp_metadata['precision@3']*exp_metadata['recall@3'])/(exp_metadata['precision@3']+exp_metadata['recall@3'])\n",
    "exp_metadata['F1@5'] = 2*(exp_metadata['precision@5']*exp_metadata['recall@5'])/(exp_metadata['precision@5']+exp_metadata['recall@5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GCN experiments:  21\n"
     ]
    }
   ],
   "source": [
    "# GCN\n",
    "gcn_mask = ( (exp_metadata['Epochs']==20) &  (exp_metadata['Model'].str.startswith('GCN')))\n",
    "print('Total GCN experiments: ',sum(gcn_mask))\n",
    "#exp_metadata[gcn_mask][['time_stamp', 'Epochs', 'Learning Rate', 'hidden_dimension','num_layers', 'num_bases', 'Model', 'hits@1', 'hits@3', 'hits@5', 'mrr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GCN experiments:  22\n"
     ]
    }
   ],
   "source": [
    "# R-GAT\n",
    "rgat_mask = ((exp_metadata['Epochs']==20) & (exp_metadata['num_bases'].isna()) & (exp_metadata['Model'].str.startswith('RGAT')))\n",
    "print('Total GCN experiments: ',sum(rgat_mask))\n",
    "#exp_metadata[mask][['time_stamp', 'Epochs', 'Learning Rate', 'hidden_dimension','num_layers', 'num_bases', 'Model', 'hits@1', 'hits@3', 'hits@5', 'mrr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total R-GCN experiments:  24\n"
     ]
    }
   ],
   "source": [
    "# R-GCN\n",
    "rgcn_mask = ((exp_metadata['Epochs']==20) & (exp_metadata['num_bases'].isna()) & (exp_metadata['Model'].str.startswith('RGCN')))\n",
    "print('Total R-GCN experiments: ',sum(rgcn_mask))\n",
    "#exp_metadata[rgcn_mask][['time_stamp', 'Epochs', 'Learning Rate', 'hidden_dimension','num_layers', 'num_bases', 'Model', 'hits@1', 'hits@3', 'hits@5', 'mrr']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sem(exp_data, mask, col):\n",
    "\n",
    "    sd = np.std(exp_data[mask][col])\n",
    "    mean = np.mean(exp_data[mask][col])\n",
    "    sem = sd/np.sqrt(len(exp_data[mask][col]))\n",
    "    print(f'{col} Mean : {np.round(mean,3)} ± {np.round(sem,3)}')\n",
    "    return np.round(mean,3),np.round(sem,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits@1 Mean : 0.225 ± 0.016\n",
      "hits@3 Mean : 0.422 ± 0.019\n",
      "hits@5 Mean : 0.541 ± 0.026\n",
      "mrr Mean : 0.329 ± 0.017\n",
      "F1@1 Mean : 0.191 ± 0.014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.191, 0.014)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GCN results\n",
    "mean_sem(exp_metadata, gcn_mask, 'hits@1')\n",
    "mean_sem(exp_metadata, gcn_mask, 'hits@3')\n",
    "mean_sem(exp_metadata, gcn_mask, 'hits@5')\n",
    "mean_sem(exp_metadata, gcn_mask, 'mrr')\n",
    "mean_sem(exp_metadata, gcn_mask, 'F1@1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits@1 Mean : 0.833 ± 0.01\n",
      "hits@3 Mean : 0.909 ± 0.004\n",
      "hits@5 Mean : 0.923 ± 0.003\n",
      "mrr Mean : 0.818 ± 0.006\n",
      "F1@1 Mean : 0.709 ± 0.008\n",
      "F1@3 Mean : 0.506 ± 0.003\n",
      "F1@5 Mean : 0.375 ± 0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.375, 0.001)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#R-GCN results\n",
    "mean_sem(exp_metadata, rgcn_mask, 'hits@1')\n",
    "mean_sem(exp_metadata, rgcn_mask, 'hits@3')\n",
    "mean_sem(exp_metadata, rgcn_mask, 'hits@5')\n",
    "mean_sem(exp_metadata, rgcn_mask, 'mrr')\n",
    "mean_sem(exp_metadata, rgcn_mask, 'F1@1')\n",
    "mean_sem(exp_metadata, rgcn_mask, 'F1@3')\n",
    "mean_sem(exp_metadata, rgcn_mask, 'F1@5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits@1 Mean : 0.641 ± 0.013\n",
      "hits@3 Mean : 0.85 ± 0.007\n",
      "hits@5 Mean : 0.894 ± 0.005\n",
      "mrr Mean : 0.716 ± 0.009\n",
      "F1@1 Mean : 0.546 ± 0.011\n",
      "F1@3 Mean : 0.451 ± 0.005\n",
      "F1@5 Mean : 0.349 ± 0.002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.349, 0.002)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#R-GAT results\n",
    "mean_sem(exp_metadata, rgat_mask, 'hits@1')\n",
    "mean_sem(exp_metadata, rgat_mask, 'hits@3')\n",
    "mean_sem(exp_metadata, rgat_mask, 'hits@5')\n",
    "mean_sem(exp_metadata, rgat_mask, 'mrr')\n",
    "mean_sem(exp_metadata, rgat_mask, 'F1@1')\n",
    "mean_sem(exp_metadata, rgat_mask, 'F1@3')\n",
    "mean_sem(exp_metadata, rgat_mask, 'F1@5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((exp_metadata['Epochs']==2) & (exp_metadata['num_bases'].isna()) & (exp_metadata['Model'].str.startswith('RGAT')))\n",
    "exp_metadata[mask][['time_stamp', 'Epochs', 'Learning Rate', 'hidden_dimension',\n",
    "       'num_layers', 'num_bases', 'Model', 'hits@1', 'hits@3', 'hits@5', 'mrr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#20230830082302  -- 30 bases\n",
    "#20230830080104 -- 80 bases\n",
    "#20230825123318  -- 193 bases\n",
    "RGCNmodel_with_193_bases = load_model('./core/experiments/qa/results/20230825123318/RGCNmodel.pt')\n",
    "RGCNmodel_with_80_bases = load_model('./core/experiments/qa/results/20230830080104/RGCNmodel.pt')\n",
    "RGCNmodel_with_30_bases = load_model('./core/experiments/qa/results/20230830082302/RGCNmodel.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGCNmodel_with_193_bases.eval()\n",
    "# Iterate through the parameters and identify base matrices\n",
    "for param in RGCNmodel_with_193_bases.parameters():\n",
    "    print(param.shape)\n",
    "\n",
    "total_params_193_bases = sum(p.numel() for p in RGCNmodel_with_193_bases.parameters())\n",
    "print(f\"Total number of parameters with 193 bases matrices: {total_params_193_bases}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGCNmodel_with_80_bases.eval()\n",
    "\n",
    "\n",
    "# Iterate through the parameters and identify base matrices\n",
    "for param in RGCNmodel_with_80_bases.parameters():\n",
    "    print(param.shape)\n",
    "\n",
    "total_params_80_bases = sum(p.numel() for p in RGCNmodel_with_80_bases.parameters())\n",
    "print(f\"Total number of parameters with 80 bases matrices: {total_params_80_bases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGCNmodel_with_30_bases.eval()\n",
    "\n",
    "# Iterate through the parameters and identify base matrices\n",
    "for param in RGCNmodel_with_30_bases.parameters():\n",
    "    print(param.shape)\n",
    "\n",
    "total_params_30_bases = sum(p.numel() for p in RGCNmodel_with_30_bases.parameters())\n",
    "print(f\"Total number of parameters with 30 bases matrices: {total_params_30_bases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(['30_bases','80_bases','193_bases'],[total_params_30_bases,total_params_80_bases,total_params_193_bases])\n",
    "plt.xlabel('#Bases')\n",
    "plt.ylabel('Model Parameter Count')\n",
    "plt.title('Bases Vs Model parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGCNmodel_with_193_bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psmg-imkg-gnn-qa-b42qFiTl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
